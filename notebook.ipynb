{
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ad8c81e83ff1f0795fa653647925aa04d75cccc0",
        "_cell_guid": "98855ef9-e4c5-4abe-99ae-2735216530ca"
      },
      "source": [
        "**EXPLORATORY DATA ANALYSIS (EDA)**\n",
        "\n",
        "\n",
        "**1) Import Data**\n",
        "\n",
        "Getting the data in the first place is a big challenge for data scientists. Luckily, Kaggle gives us a nice data set to use for the competition so we are just going to import the CSV file using pandas.\n",
        "\n",
        "From Kaggle we get 2 data sets : \"train\" and \"test\". The test set is there only to validate our predictive model at the end of the study but we are going to import it at the same time so it's done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "34616fd30c7ade8c3289d3421c06557492eb6382",
        "collapsed": true,
        "_cell_guid": "ece46aec-c3b8-4ab4-92da-0cf4c06b7e31"
      },
      "execution_count": null,
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv(\"../input/train.csv\")\n",
        "test = pd.read_csv(\"../input/test.csv\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fac338b9a666c065056e42a3053e93ed63af64e0",
        "_cell_guid": "832383b8-2f0c-463f-9d3f-54d5d632597a"
      },
      "source": [
        "Usually the first thing we want to do is have a quick glance at our data. For this we have the attribute pd.shape and 2 basic methods : pd.head() and pd.describe()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "91d10aa5b16044374e30cc280f6e6d2a0d8a8810",
        "_cell_guid": "b0db53cc-bdaf-4551-a244-4ed5622a4b69"
      },
      "execution_count": null,
      "source": [
        "train.shape"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d99fcdb0eabd4d1d18b6c2237af02100088964a7",
        "_cell_guid": "8bcd2145-3b02-4b6b-b96c-e469c1f107f9"
      },
      "execution_count": null,
      "source": [
        "train.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0d3681ed5a0fd5cca3130d5e413d39dd141195c0",
        "_cell_guid": "c1307804-47a3-4ad0-b874-342a89382272"
      },
      "execution_count": null,
      "source": [
        "train.describe(include = 'all') ## pro tip : add \"include = \"all\"\" to show the non numerical columns"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3a51a76469dcfc47f80d926e5978549f2cedf5ba",
        "_cell_guid": "c588df76-3533-46e8-a727-97beb2cbb069"
      },
      "source": [
        "Alright, so far we know that our data set has 891 rows and 12 columns :\n",
        "\n",
        "* PassengerId\n",
        "* Survived \n",
        "* Pclass: the passenger class. It has three possible values: 1,2,3\n",
        "* The Name\n",
        "* The Sex\n",
        "* The Age\n",
        "* SibSp: number of siblings and spouses traveling with the passenger\n",
        "* Parch: number of parents and children traveling with the passenger\n",
        "* The ticket number\n",
        "* The ticket Fare\n",
        "* The cabin number\n",
        "* The embarkation. It has three possible values S,C,Q\n",
        "\n",
        "We can see that we have only 204 out of 891 values for Cabin, that's not enough to take into consideration so we are going to drop this feature. Also, we can see from train.head() that the ticket number and PassengerId features seem useless so we're going to delete them aswell. \n",
        "\n",
        "We also have missing values for Age. We could try to guess the missing ages based on other features but for the EDA we're just going to drop the rows with a NaN value using pd.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "592a1fad68011f3d056168aa5c645f5833c8b426",
        "_cell_guid": "66599c0b-1c06-4f8c-8bdc-3f0ba39091d6"
      },
      "execution_count": null,
      "source": [
        "train = train.drop(['Ticket','Cabin','PassengerId'], axis=1)\n",
        "train = train.dropna()\n",
        "train['Sex'] = train['Sex'].map({'male':1,'female':0})\n",
        "train.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f0e9612d496729435f6b57bd474292e5a4e51f71",
        "_cell_guid": "afbe88af-82ed-4f42-8176-bc1b8540321a"
      },
      "source": [
        "Now we're going to have a look at our data using graphs using  matplotlib and seaborn :\n",
        "Let's start with our Survived feature distribution : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "69c51e32ae020932bbf4ca1cd7e587344466ea43",
        "collapsed": true,
        "_cell_guid": "7dae6b09-a556-4307-8cdf-66e0fc9c86a1"
      },
      "execution_count": null,
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('white')"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "26daee982ac3fb34763e40d62b1bf7cc2d6726d9",
        "_cell_guid": "cdd41e05-b1ce-4465-8d49-8ef641e3f238"
      },
      "source": [
        "Let's start with our Survived feature distribution : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2ca11677a757dcf358e0a8f4de5be459f63802d8",
        "_cell_guid": "5407caa2-a000-45bc-b9bb-f690ddd7abff"
      },
      "execution_count": null,
      "source": [
        "g = sns.countplot(\"Survived\", data = train)\n",
        "plt.title(\"Survived distribution on the dataset\")\n",
        "sns.plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3de4bd69790e20b47bbd44dbcb7d04527c14cf12",
        "_cell_guid": "0152c179-530f-44d4-85e0-432d60a64485"
      },
      "source": [
        "We can see that in our dataset more people died than survived on the Titanic. \n",
        "\n",
        "Next we are going to create a correlation heatmap. Correlation is a statistical technique that is used to measure and describe the strength and direction of the relationship between two variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e2025d2a35ec41f3c5f2e753780f6c47d5baf1ad",
        "_cell_guid": "4433662e-2762-4d61-9f64-fbd2f694990c"
      },
      "execution_count": null,
      "source": [
        "g = sns.heatmap(train.corr(), annot=True, fmt=\".2f\")\n",
        "plt.title(\"Heatmap of the correlations\")\n",
        "sns.plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "89f5b25aea620a9a54b413a7e7bd4383c66f4aba",
        "_cell_guid": "44f00e34-2b3b-4511-898f-e8565936cd2d"
      },
      "source": [
        "Here we can see positive correlations between :\n",
        "* ** Survived and Fare **: People who paid more for their ticket tend to have a higher Survived rate.\n",
        "* **Parch and Sibsp** : Sibsp is the number of siblings +spouses and Parch is the number of parents and children so it makes sense that the 2 are correlated.\n",
        "* **Parch and Fare** : People who have a big family pay more for their ticket, because usually if you can afford to travel with a big family you can afford an expensive ticket.\n",
        "\n",
        "And negative correlations between :\n",
        "* **Survived and Sex** : Men have a Survived rate way lower than women, thus \"Women and child first!\"\n",
        "* **Survived and Pclass** : Our classes go from 1 to 3 with 3 being the cheapest and 1 the most expensive. Here a negative correlation means that people who are in the third class tend to have a lower Survived rate. \n",
        "* **Pclass and Fare** : The class is determined by the price of the ticket.\n",
        "* **Pclass and Age** : The older people tend to be in the most expensive classes.\n",
        "\n",
        "\n",
        "\n",
        "Let's have a more precize look of these features to confirm our insuptions : \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1a1f20b4407b0a5bdd9cee5651e50b56e9fbf525",
        "_cell_guid": "e1f1ee93-0053-4690-a586-e0f802ddb2d9"
      },
      "execution_count": null,
      "source": [
        "g = sns.factorplot(\"Pclass\",\"Survived\", hue=\"Sex\", kind = \"bar\", data=train, ci = None)\n",
        "plt.title(\"Survived rate based on class and sex\")\n",
        "sns.plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8ba34c3a0eb27215da7b7747ad5d558ec8723139",
        "_cell_guid": "d6eae1a0-637e-4858-a2c9-5a8781e312b9"
      },
      "execution_count": null,
      "source": [
        "train.groupby('Pclass').mean()['Fare'].plot(kind='bar',figsize=(12,6))\n",
        "plt.title(\"Average fare for each class\")\n",
        "sns.plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cbf6101f12ccb7e4016657b5c696991cbb3d9fce",
        "_cell_guid": "21674a6e-6b3b-4b59-86b1-982a36b7e879"
      },
      "execution_count": null,
      "source": [
        "fig = sns.FacetGrid(train,hue='Pclass',aspect=4)\n",
        "fig.map(sns.kdeplot,'Age',shade=True)\n",
        "plt.title(\"Age distribution for each class\")\n",
        "fig.add_legend()\n",
        "sns.plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "80fd9118bc9e6bb44e0f1314951384e901004078",
        "collapsed": true,
        "_cell_guid": "00a3316e-9d7f-4787-8ee5-c311b37fb04d"
      },
      "source": [
        "Alright, we took a glance at the data and spotted some interesting correlations. However, we couldn't manage to analyze more complicated features like the names because these required further processing. This is why in the next part we'll focus on the ways to transform these features to fit our machine learning algorithms.\n",
        "\n",
        "****FEATURE ENGINEERING****\n",
        "\n",
        "Feature engineering is the process of reshaping, transforming or creating new features based on the previous ones. We're going to do it on both data sets, so to save time we'll combine the train and test set together :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f8833a172d57b4f10664b3c062f3a8feb6aba8ce",
        "_cell_guid": "a9f412aa-8cae-484e-bd46-a1412348cc94"
      },
      "execution_count": null,
      "source": [
        "train = pd.read_csv(\"../input/train.csv\")\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "\n",
        "data = train.append(test)\n",
        "data.shape"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f755e16642366a48c41c1372c71da7dc4b22d95e",
        "_cell_guid": "eb76de51-8c69-4b18-8611-d9a00370a119"
      },
      "execution_count": null,
      "source": [
        "data.describe(include = \"all\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8ce445b0c444146ae6ea982756887149805534e1",
        "_cell_guid": "851f39de-e3c3-40de-a71f-0bf5ec294d7a"
      },
      "execution_count": null,
      "source": [
        "data.info()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "14d1fe629f7ac54aef650d5a9ab2ce3c9523bb43",
        "_cell_guid": "84f7f406-6b0d-4b75-be50-8ddca343197e"
      },
      "source": [
        "For the same reason as mentionned at the begining, we're going to drop our useless / unusable features : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e7291bce4f5e208cf3daa347dd9a1ceb93be0c2a",
        "collapsed": true,
        "_cell_guid": "e14af905-7a14-406e-b425-9f783f6908d3"
      },
      "execution_count": null,
      "source": [
        "data = data.drop(['Survived','Ticket','Cabin','PassengerId'], axis=1)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4f33f8cec250abe9b0cece8ea053077d8ab0920a",
        "_cell_guid": "87ac5402-ca8c-41f5-a3b0-d2fa37a2cb60"
      },
      "source": [
        "Processing Name : if we look closely at the Name feature, we can see that each name has title in it : for example \"Braund, Mr. Owen Harris\" has the title \"Mr\". We can group these title into categories  and create a new feature. \n",
        "\n",
        "So first, we're breaking each name using the first \",\" and the \".\" and extract the title. There's a lot of them so we then proceed to group them into 5 categories :\n",
        "* Officer\n",
        "* Royalty\n",
        "* Mrs\n",
        "* Miss\n",
        "* Master"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5717cc3a0afcfb9deb2edcd7a62e41d382e4dd97",
        "collapsed": true,
        "_cell_guid": "392ea45a-ab43-46bd-bf2b-2563a3909b86"
      },
      "execution_count": null,
      "source": [
        "def get_title(name):\n",
        "    if '.' in name:\n",
        "        return name.split(',')[1].split('.')[0].strip()\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "    \n",
        "Title_Map = {\n",
        "                        \"Capt\":       \"Officer\",\n",
        "                        \"Col\":        \"Officer\",\n",
        "                        \"Major\":      \"Officer\",\n",
        "                        \"Jonkheer\":   \"Royalty\",\n",
        "                        \"Don\":        \"Royalty\",\n",
        "                        \"Sir\" :       \"Royalty\",\n",
        "                        \"Dr\":         \"Officer\",\n",
        "                        \"Rev\":        \"Officer\",\n",
        "                        \"the Countess\":\"Royalty\",\n",
        "                        \"Dona\":       \"Royalty\",\n",
        "                        \"Mme\":        \"Mrs\",\n",
        "                        \"Mlle\":       \"Miss\",\n",
        "                        \"Ms\":         \"Mrs\",\n",
        "                        \"Mr\" :        \"Mr\",\n",
        "                        \"Mrs\" :       \"Mrs\",\n",
        "                        \"Miss\" :      \"Miss\",\n",
        "                        \"Master\" :    \"Master\",\n",
        "                        \"Lady\" :      \"Royalty\"\n",
        "\n",
        "                        }\n",
        "    \n",
        "data[\"Title\"] = data[\"Name\"].apply(get_title).map(Title_Map)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1e24caf8f31101c6c04ef1dd8437af225a2a12aa",
        "_cell_guid": "dd6d1956-f63a-4264-b2a8-f0503ee58973"
      },
      "source": [
        "We now have a Title for each of our passenger. \n",
        "\n",
        "Processing Age : We're missing a lot of value for our Age feature and we can't just drop the columns with a missing value because it would hurt our modele accuracy to much. We could just use the mean or the median of all the ages but we can do better here.\n",
        "We can group our passengers by sex, class and our newly created feature title and get the median age of each group, and then replace our missing values based on the other features for each passenger\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "bc70643a2ab4856f20559d48395579e78c9122ed",
        "_cell_guid": "a410b07d-51c6-4d86-9b3e-bb4cd6b3c1a6"
      },
      "execution_count": null,
      "source": [
        "data[\"Age\"] = data.groupby(['Sex','Pclass','Title'])['Age'].transform(lambda x: x.fillna(x.median()))\n",
        "data.info()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f0806a10f58cb5830864a5e8b3f99fe4c65c1b8b",
        "_cell_guid": "0b87e8d6-6c4d-4305-a5f6-1fc6d726b44c"
      },
      "source": [
        "This single line of code does all the job, if you have trouble understanding it you can check the well made documentation here : https://pandas.pydata.org/pandas-docs/stable/groupby.html\n",
        "\n",
        "Processing title :\n",
        "\n",
        "Modeles usually need  numerical variables, this is why when we have categorical variables we need to transform them.\n",
        "One way to do it is with dummy encoding. For example here we have one categorical feature \"Title\" with 5 categories as strings. The function pd.get_dummies will create a column for each categorie and fill it with 0 and 1, 1 meaning that the passenger is in this category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "97fdcba145f65843caf1f9f5f4a21791b97333dd",
        "_cell_guid": "ce33a213-3c72-4d56-b178-6f68542c2c10"
      },
      "execution_count": null,
      "source": [
        "titles_dummies = pd.get_dummies(data['Title'],prefix='Title')\n",
        "data = pd.concat([data,titles_dummies],axis=1)\n",
        "\n",
        "data.drop(\"Name\", axis=1, inplace = True)\n",
        "data.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d0beeb0fad88f6ab767636aca7f5bbe2044e62e8",
        "_cell_guid": "12832c50-ee0c-4833-ac3a-d34d9c4a6834"
      },
      "source": [
        "Processing Fare : \n",
        "We have a few missing values here to we're just going to fill them with the mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cf78f0dfbd1f29a59f601a356bd0ff6b7bd72868",
        "collapsed": true,
        "_cell_guid": "dbe53d9b-5fa7-464b-882b-967534152b29"
      },
      "execution_count": null,
      "source": [
        "data.Fare.fillna(data.Fare.mean(), inplace=True)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "75d388af083e65f11fb70b4662de88affd719bf1",
        "_cell_guid": "87d5d3c4-5a37-4bde-bc6a-9c172ce92467"
      },
      "source": [
        "Processing Embarked :\n",
        "We also have a few missing values so we're going to replace it with the most frequent one : S, and then get the dummy columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f92b9dec346e3001765b84160c7199dfbcf450af",
        "collapsed": true,
        "_cell_guid": "96657d01-a132-4ae2-a7ca-8d9a7756a309"
      },
      "execution_count": null,
      "source": [
        "data.Embarked.fillna('S', inplace=True)\n",
        "Embarked_dummies = pd.get_dummies(data['Embarked'],prefix='Embarked')\n",
        "data = pd.concat([data,Embarked_dummies],axis=1)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "43c7924233ea9d558cacead5d4af21ad404d2db8",
        "_cell_guid": "46e28a31-d30f-45b1-9919-8c19e9e29f8e"
      },
      "source": [
        "Processing Parch and Sibsp :\n",
        "We're going to use Parch and Sibsp  to create a new one called \"Familysize\" that will basicly be the addition of these 2 features plus the passenger.\n",
        "Then we will break it into 3 categories : singleton (passenger alone), small family and large family."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "46ed010478ebfe6b4f1bb3fac5edc9e443c71c2d",
        "collapsed": true,
        "_cell_guid": "9c89920c-8d8d-4873-b68a-6d6c690ec813"
      },
      "execution_count": null,
      "source": [
        "data['FamilySize'] = data['Parch'] + data['SibSp'] + 1\n",
        "\n",
        "data['Singleton'] = data['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
        "data['SmallFamily'] = data['FamilySize'].map(lambda s: 1 if 2<=s<=4 else 0)\n",
        "data['LargeFamily'] = data['FamilySize'].map(lambda s: 1 if 5<=s else 0)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "46c7981fb2a873bf5b997ce15ca4ef076c19b5fd",
        "_cell_guid": "8bc57996-e34c-4275-b4b0-5656c3023223"
      },
      "source": [
        "Processing Sex :\n",
        "Here we're just going to transform our strings into a numerical variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b69ff3fa037c3488544f31aaa76da12eef586274",
        "collapsed": true,
        "_cell_guid": "7e90dd7a-52cf-40cb-85bf-4c7038ae86a8"
      },
      "execution_count": null,
      "source": [
        "data['Sex'] = data['Sex'].map({'male':1,'female':0})"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2465871517df6d3619b660772e69bb2e57a56caf",
        "_cell_guid": "eab07305-1710-49ff-b91f-e52d2a5b6929"
      },
      "source": [
        "Processing Pclass : \n",
        "Just dummy encoding our feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "173b75fd377efe9f887535b1361ec4b9c921a14f",
        "collapsed": true,
        "_cell_guid": "e6225ad1-0450-4426-96ca-ef6e32149a5c"
      },
      "execution_count": null,
      "source": [
        "pclass_dummies = pd.get_dummies(data['Pclass'], prefix=\"Pclass\") \n",
        "data = pd.concat([data,pclass_dummies],axis=1)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8026ea3559a1735d8e4ab6675bb772648b206170",
        "_cell_guid": "dcf6d105-c254-4a98-a1b4-bda2e8cc1109"
      },
      "source": [
        "Alright, we're almost there. Now we're going to drop the old features that are useless now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d0aceedeb37d97a4579bf3e3a1b41b6b7e5ddd01",
        "_cell_guid": "8ec3d86a-1430-4e58-81d1-37b93842b47d"
      },
      "execution_count": null,
      "source": [
        "data.drop(['Pclass','Embarked','Title'],axis=1,inplace=True)\n",
        "data.shape"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6213ebef837507729bee1f88a775ff880c1d19e9",
        "_cell_guid": "b4fc3b8c-3a5c-4fa1-b5c6-ba1d9f10491c"
      },
      "source": [
        "****MODELING****\n",
        "\n",
        "This is the exciting part where we are going to make predictions using our data. The first thing we want to do is splitting back our data into a train set and a test set. It is a crucial part because we need a way to evaluate our modele at the end to get a score for Kaggle.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1449283913050c53ccbdf6ccbaecff18113e16e3",
        "collapsed": true,
        "_cell_guid": "93bdd16a-8bd7-43b4-9716-e25072c1b78a"
      },
      "execution_count": null,
      "source": [
        "train0 = pd.read_csv(\"../input/train.csv\")\n",
        "targets = train0.Survived\n",
        "train = data.head(891) #when we created our data set \"data\" we basicly put our test dataframe at \n",
        "                       #the end of the train one, so we can split it just by selecting the values before 891  \n",
        "test = data.iloc[891:]\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "00ba6f54f2d2c93448d46baa9b445884d1dd3768",
        "_cell_guid": "7f5001d0-ff10-41c2-8ba4-1651bc01bf48"
      },
      "source": [
        "Alright so here we have a classification problem : we're trying to predict a categorical response \"Suvived\" which is translated into a 0 or a 1. There are many classification modeles we can use with the Scikit-learn package but first we need a way to evaluate our modele performance.\n",
        "Since we can't test our modele on the same data it was trained, we have to split our train dataframe again. It might be confusing but to clarify : we first split train and set only for the Kaggle challenge score and then we do it to evaluate the modele for ourselves.\n",
        "\n",
        "We're going to use cross validation from Sklearn : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2b8ac74aabbb7925b4624eec7324e39920368dff",
        "collapsed": true,
        "_cell_guid": "fa163362-663d-4e41-a0f2-c5967db8d8fc"
      },
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "def compute_score(classifier, X, y, scoring='accuracy'):\n",
        "    xval = cross_val_score(classifier, X, y, cv = 5, scoring=scoring)\n",
        "    return np.mean(xval)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "17d9f328ac31553d4c14dbc21a483ce3ecb215ea",
        "_cell_guid": "5d0973dd-1f3c-466b-a922-aab1bf336f83"
      },
      "source": [
        "Now that we have our tool to evaluate our model, let's start with one of the simplest classification : KNN. You can learn more about KNN here : http://scikit-learn.org/stable/modules/neighbors.html\n",
        "\n",
        "First we import the model from Sklearn.neighbors, then make an instance of our estimator (=model). This is where we enter the parameters we want, here we're going to select K=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5fea7bcf737d6ef4317ca8366a5fee3153d8c9b9",
        "_cell_guid": "180a9d84-4e59-493e-b303-72c904102966"
      },
      "execution_count": null,
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "knn"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a4aab9c2a12884448da02bd145ebe1579fc9e1cb",
        "_cell_guid": "845f3481-a802-472e-b56c-a623c2c09a18"
      },
      "source": [
        "    Alright now we fit the model on our data and use our compute_score to get an accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d14fdc170e81fd0fa739b1c604c350b5538ac95c",
        "_cell_guid": "fdecbed8-c33d-4a24-83d7-1e04e36e0d8d"
      },
      "execution_count": null,
      "source": [
        "knn.fit(train,targets)\n",
        "knn_score = compute_score(knn,train,targets)\n",
        "knn_score"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8f39e371f5ef90cb46e7bf0cf0ac45c7affca741",
        "_cell_guid": "a7f7ff35-2863-4d84-9879-088366654c84"
      },
      "source": [
        "This is our result for K=1.\n",
        "Now let's try for different values of K"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d8bc5140ca3cd2cb801182dca9de1c88091b83e0",
        "_cell_guid": "aa0a7678-a9b9-4e9a-8b7f-4bb3c1c7ddb7"
      },
      "execution_count": null,
      "source": [
        "k_range = list(range(1, 31))\n",
        "k_scores = []\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = compute_score(knn, train,targets)\n",
        "    k_scores.append(scores.mean())\n",
        "print(k_scores)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "65800b71f9f2329d21fd52fc4ab44d43508ecee2"
      },
      "source": [
        "We plot our results :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "64565a146ff9cb709cb1b1c3bf3eec84b089c891",
        "_cell_guid": "7912f51c-1ef4-4eaf-acd0-ed52af355580"
      },
      "execution_count": null,
      "source": [
        "plt.plot(k_range, k_scores)\n",
        "plt.xlabel('Value of K for KNN')\n",
        "plt.ylabel('Cross-Validated Accuracy')"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4a431d49f410461b3dfec405718dd142e77e958d"
      },
      "execution_count": null,
      "source": [
        "knn.best_score_"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b68107281e93ebdc2311b2ceefe8d74537161bf7"
      },
      "source": [
        "We can see that the best accuracy with this model is with K=21. \n",
        "\n",
        "There's a tool we can use to test every combination for each parameters and find the best one : GridSearchCV. We feed it with the map of parameters we want it to test and then fit our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "071d1cca65322d3fc1b7a01b9d8cf0e58d2b7898",
        "_cell_guid": "6fbe0944-2274-4084-8907-82113afceea7"
      },
      "execution_count": null,
      "source": [
        "from sklearn.grid_search import GridSearchCV\n",
        "k_range = list(range(1, 31))\n",
        "param_grid = dict(n_neighbors=k_range)\n",
        "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(train, targets)\n",
        "\n",
        "grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
        "plt.plot(k_range, grid_mean_scores)\n",
        "plt.xlabel('Value of K for KNN')\n",
        "plt.ylabel('Cross-Validated Accuracy')\n",
        "\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d4b63e7acc0954f1c17a1a357f746afb36d05cfc"
      },
      "source": [
        "We notice that this plot is the same as the previous one.\n",
        "\n",
        "Alright now let's try a more complex model : Random forest classifier. Random forest uses decision trees. You can learn more about it here : https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
        "\n",
        "Let's try it with the basic parameters : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f894824e07832d5faf4c7911f1fd110abbe6a60d",
        "_cell_guid": "fcf7c9fc-8c8d-4fb0-b894-032857030573"
      },
      "execution_count": null,
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rdc = RandomForestClassifier()\n",
        "rdc.fit(train,targets)\n",
        "rdc_score = compute_score(rdc,train,targets,scoring='accuracy')\n",
        "rdc_score"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e6a73a976a7a90a55a69c657b5e1e65cfd0c424c"
      },
      "source": [
        "Now we use GridSearchCV to find the best combination of parameters : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9e47e01db75be3a3558b329b05cbd429563de271",
        "_cell_guid": "b2d0a26f-75da-4a2f-a7a6-ea81cc7045dc"
      },
      "execution_count": null,
      "source": [
        "run_grid = False\n",
        "if run_grid :\n",
        "    param_grid = {'max_depth' : [4, 6, 8],\n",
        "                 'n_estimators': [50, 10],\n",
        "                 'max_features': ['sqrt', 'auto', 'log2'],\n",
        "                 'min_samples_split': [2, 3, 10],\n",
        "                 'min_samples_leaf': [1, 3, 10],\n",
        "                 'bootstrap': [True, False],}\n",
        "    grid = GridSearchCV(rdc, param_grid, cv=10, scoring='accuracy')\n",
        "    grid.fit(train, targets)\n",
        "    grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
        "    model = grid\n",
        "    parameters = grid.best_params_\n",
        "    print(grid.best_score_)\n",
        "    print(grid.best_params_)\n",
        "else : \n",
        "    parameters = {'bootstrap': False, 'max_depth': 6, 'max_features': 'auto', 'min_samples_leaf': 3,\n",
        "              'min_samples_split': 3, 'n_estimators': 10}\n",
        "    model = RandomForestClassifier(**parameters)\n",
        "    model.fit(train,targets)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "92cfee35259d4a48bf2f6e78e3295760bc367271"
      },
      "execution_count": null,
      "source": [
        "rfc_score = compute_score(model, train, targets, scoring='accuracy')\n",
        "rfc_score"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c30467c2dee4d01bb029426120fc53230a80b410"
      },
      "source": [
        "82.8%, that's better ! This number is relative to the problem so we can't say if it is good or bad. The goal of data scientist is to improve this result by :\n",
        "- creating new features\n",
        "- try different models  such as Gradient Boosted trees, XGboost"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "nbconvert_exporter": "python"
    }
  },
  "nbformat_minor": 1
}